Datadog is a cloud-based monitoring and analytics platform that provides full-stack observability and helps organizations monitor their applications, infrastructure, and user experiences in real-time. It unifies metrics, logs, and traces into dashboards to help teams troubleshoot issues faster and optimize performance. I worked on LLM Observability, Datadog's first product for LLM-native applications. With LLM Observability, organizations can monitor, troubleshoot, and evaluate their applications (e.g. investigate root cause of issues, monitor real-time performance, and evaluate the quality, privacy, and safety of their applications). 

LLM Observability's out-of-the-box (OOTB) evaluations are built-in tools that assess applications on dimensions like quality, security, and safety. Organizations can enable and configure evals to assess the effectiveness of their applications' responses, including detection of sentiment, topic relevancy, toxicity, failure to answer, and hallucination. While I was on the team, I successfully designed and shipped the end-to-end eval pipeline for prompt injection, beating all internal benchmarks. This involved the 

